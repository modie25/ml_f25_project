{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis and Data Preparation\n",
        "\n",
        "This notebook presents a comprehensive exploratory data analysis (EDA) and data preparation pipeline for the red wine quality classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Overview and Initial Analysis\n",
        "\n",
        "We begin our analysis with a dataset containing 1,599 wine samples and 11 physicochemical features, along with a target variable representing wine quality. The features include fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol. We performed an initial analysis and found that there were no missing values in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better-looking plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# The CSV header has a special format with quotes, so we need to parse it carefully\n",
        "with open('data/raw/winequality-red.csv', 'r') as f:\n",
        "    header_line = f.readline().strip()\n",
        "\n",
        "# Extract column names from the header\n",
        "header_clean = header_line.replace('\"\"', '\"').replace('\"', '')\n",
        "columns = [col.strip() for col in header_clean.split(';')]\n",
        "\n",
        "# Read the data\n",
        "df = pd.read_csv('data/raw/winequality-red.csv', sep=';', skiprows=1, names=columns)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Total features: {len(df.columns) - 1}\")\n",
        "print(f\"\\nFeature names:\")\n",
        "for i, col in enumerate(df.columns[:-1], 1):\n",
        "    print(f\"{i:2d}. {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing = df.isnull().sum()\n",
        "print(\"Missing values per column:\")\n",
        "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution of feature values is summarized in the following table. An additional visualization can be found in the appendix (feature distributions) that allows us to understand the distribution of each feature, including symmetry, potential outliers, and other characteristics, which complements the table and can be verified by the reader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary of all features\n",
        "statistical_summary = df.describe()\n",
        "\n",
        "# Create a more beautiful table (stargazer style)\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "ax.axis('off')\n",
        "\n",
        "# Prepare data\n",
        "table_data = statistical_summary.round(2).values\n",
        "row_labels = ['Count', 'Mean', 'Std Dev', 'Min', '25%', 'Median', '75%', 'Max']\n",
        "col_labels = statistical_summary.columns.tolist()\n",
        "\n",
        "# Create table with better styling\n",
        "table = ax.table(cellText=table_data,\n",
        "                 rowLabels=row_labels,\n",
        "                 colLabels=col_labels,\n",
        "                 cellLoc='center',\n",
        "                 loc='center',\n",
        "                 bbox=[0, 0, 1, 1])\n",
        "\n",
        "# Style the table\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(8)\n",
        "\n",
        "# Header row styling\n",
        "for i in range(len(col_labels)):\n",
        "    cell = table[(0, i)]\n",
        "    cell.set_facecolor('#4472C4')\n",
        "    cell.set_text_props(weight='bold', color='white')\n",
        "    cell.set_height(0.08)\n",
        "\n",
        "# Row labels styling\n",
        "for i in range(len(row_labels)):\n",
        "    cell = table[(i+1, -1)]\n",
        "    cell.set_facecolor('#E7E6E6')\n",
        "    cell.set_text_props(weight='bold')\n",
        "    cell.set_width(0.12)\n",
        "\n",
        "# Alternate row colors for better readability\n",
        "for i in range(len(row_labels)):\n",
        "    for j in range(len(col_labels)):\n",
        "        cell = table[(i+1, j)]\n",
        "        if i % 2 == 0:\n",
        "            cell.set_facecolor('#F2F2F2')\n",
        "        else:\n",
        "            cell.set_facecolor('white')\n",
        "        cell.set_height(0.06)\n",
        "\n",
        "ax.set_title('Statistical Summary of Features', fontsize=14, fontweight='bold', pad=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/statistical_summary.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Statistical Summary of Features\")\n",
        "print(\"=\" * 80)\n",
        "statistical_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Target Variable Analysis and Feature Correlations\n",
        "\n",
        "Wine quality serves as the target variable, taking discrete values from 3 to 8. Understanding the distribution of wine quality is essential for addressing the challenges inherent in classification tasks, particularly given the class imbalance observed in the dataset. Additionally, analyzing feature relationships is critical for identifying multicollinearity and understanding the underlying data structure that may influence model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** As a team, we read the codebook and documentation of the data (Cortez et al., 2009; UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/wine+quality) and although in the description of the dictionaries the source states that quality can take values from 0 to 10, in our dataset we only found possible values from 3 to 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable distribution\n",
        "quality_dist = df['quality'].value_counts().sort_index()\n",
        "total = len(df)\n",
        "\n",
        "# Create frequency table\n",
        "frequency_table = pd.DataFrame({\n",
        "    'Quality': quality_dist.index,\n",
        "    'Count': quality_dist.values,\n",
        "    'Percentage': (quality_dist.values / total * 100).round(2)\n",
        "})\n",
        "\n",
        "print(\"TARGET VARIABLE (QUALITY) DISTRIBUTION\")\n",
        "print(\"=\" * 60)\n",
        "print(frequency_table.to_string(index=False))\n",
        "print(f\"\\nQuality range: {df['quality'].min()} to {df['quality'].max()}\")\n",
        "print(f\"Quality values found: {sorted(df['quality'].unique())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create combined figure: quality bar chart (left) and correlation matrix (right)\n",
        "# Significantly increase size for PDF readability\n",
        "fig, axes = plt.subplots(1, 2, figsize=(24, 10), gridspec_kw={'width_ratios': [0.6, 1.6], 'wspace': 0.6})\n",
        "\n",
        "# Left: Quality distribution bar chart\n",
        "ax1 = axes[0]\n",
        "bars = ax1.bar(quality_dist.index, quality_dist.values, \n",
        "               color='crimson', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax1.set_xlabel('Quality Score', fontsize=13, fontweight='bold')\n",
        "ax1.set_ylabel('Count', fontsize=13, fontweight='bold')\n",
        "ax1.set_title('Quality Score Distribution', fontsize=15, fontweight='bold', pad=12)\n",
        "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "ax1.set_xticks(quality_dist.index)\n",
        "ax1.tick_params(axis='both', labelsize=11)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(height)}\\n({height/total*100:.1f}%)',\n",
        "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Right: Correlation matrix - make it much bigger for PDF\n",
        "correlation_matrix = df.corr()\n",
        "ax2 = axes[1]\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "heatmap = sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=0.25, \n",
        "            cbar_kws={\"shrink\": 0.8, \"aspect\": 8, \"label\": \"\"}, \n",
        "            annot_kws={'size': 9, 'weight': 'bold'}, ax=ax2)\n",
        "ax2.set_title('Correlation Matrix', fontsize=15, fontweight='bold', pad=15)\n",
        "ax2.set_xticks(range(len(correlation_matrix.columns)))\n",
        "ax2.set_xticklabels(correlation_matrix.columns, fontsize=10, rotation=90)\n",
        "ax2.set_yticks(range(len(correlation_matrix.columns)))\n",
        "ax2.set_yticklabels(correlation_matrix.columns, fontsize=10, rotation=0)\n",
        "\n",
        "# Adjust colorbar font size\n",
        "cbar = heatmap.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "# Use subplots_adjust for better control\n",
        "plt.subplots_adjust(left=0.04, right=0.98, top=0.85, bottom=0.08, wspace=0.6)\n",
        "plt.savefig('figures/quality_and_correlation_combined.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bar chart (left) reveals that this variable exhibits class imbalance, with quality levels 5 and 6 being the most frequent, respectively, while there are very few wines with low quality (such as 3) or extremely high quality (such as 8). The order of magnitude of the differences is very significant, as in our dataset there is a probability of 0.63% of having a wine with quality 3, while the probability is more than 68 times greater for quality 5 (42.59%). Similarly, quality 6 has a probability of 39.90%, which is more than 63 times greater than quality 3.\n",
        "\n",
        "The correlation matrix (right) provides a comprehensive view of pairwise linear relationships between all physicochemical features, enabling us to assess both feature-feature interactions and feature-target associations simultaneously. Notable pairwise correlations between features include strong positive correlations between fixed acidity and citric acid (0.67), fixed acidity and density (0.67), and total sulfur dioxide and free sulfur dioxide (0.67). Strong negative correlations are observed between fixed acidity and pH (-0.68), citric acid and volatile acidity (-0.55), and citric acid and pH (-0.54). These relationships indicate potential multicollinearity that may influence model performance. By examining the quality row in the correlation matrix, we identify the strongest associations with wine quality. Alcohol content shows the strongest positive correlation (0.48), followed by sulphates (0.25) and citric acid (0.23). Conversely, volatile acidity exhibits the strongest negative correlation (-0.39), indicating that higher levels are associated with lower quality scores. Other features showing moderate negative correlations include total sulfur dioxide (-0.19) and density (-0.17). While quality is an ordinal variable, we compute Pearson correlation coefficients given its natural ordering, which allows us to capture linear trends. These correlations provide an initial indication of feature importance, though the classification models will ultimately determine the true discriminative power of each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class imbalance ratios\n",
        "print(\"\\nClass Imbalance Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "for q in sorted(quality_dist.index):\n",
        "    count = quality_dist[q]\n",
        "    prob = count / total\n",
        "    print(f\"Quality {q}: {count:4d} samples, P = {prob:.4f} ({prob*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nRatio Quality 5 to Quality 3: {quality_dist[5] / quality_dist[3]:.2f}x\")\n",
        "print(f\"Ratio Quality 6 to Quality 3: {quality_dist[6] / quality_dist[3]:.2f}x\")\n",
        "print(f\"Ratio Quality 5 to Quality 8: {quality_dist[5] / quality_dist[8]:.2f}x\")\n",
        "print(f\"Ratio Quality 6 to Quality 8: {quality_dist[6] / quality_dist[8]:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Additional Visualizations\n",
        "\n",
        "### A1. Statistical Summary Table\n",
        "\n",
        "The following table provides comprehensive descriptive statistics for all features in the dataset, including count, mean, standard deviation, minimum, maximum, and quartiles.\n",
        "\n",
        "![Statistical Summary of Features](figures/statistical_summary.png)\n",
        "\n",
        "*Table A1: Statistical Summary of Features*\n",
        "\n",
        "### A2. Feature Distributions\n",
        "\n",
        "The following visualization provides a comprehensive view of the distribution of each feature, including symmetry, potential outliers, and other characteristics that complement the statistical summary table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of all features\n",
        "features = df.columns[:-1]  # All columns except 'quality'\n",
        "n_features = len(features)\n",
        "n_cols = 4\n",
        "n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 2.5*n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    axes[i].hist(df[feature], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "    axes[i].set_title(f'{feature}', fontsize=10, fontweight='bold')\n",
        "    axes[i].set_xlabel('Value', fontsize=8)\n",
        "    axes[i].set_ylabel('Frequency', fontsize=8)\n",
        "    axes[i].grid(axis='y', alpha=0.3)\n",
        "    axes[i].tick_params(labelsize=7)\n",
        "\n",
        "# Hide extra subplots\n",
        "for i in range(n_features, len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Feature Distributions', fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.99], hpad=0.3, wpad=0.3)\n",
        "plt.savefig('figures/feature_distributions.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Feature Distributions](figures/feature_distributions.png)\n",
        "\n",
        "*Figure A2: Distribution of all features showing histograms for each physicochemical property*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Correlation Analysis\n",
        "\n",
        "Understanding the relationships between features is crucial for comprehending the underlying structure of the dataset and identifying potential multicollinearity issues that may affect model performance. The correlation matrix provides a comprehensive view of pairwise linear relationships between all physicochemical features, enabling us to assess both feature-feature interactions and feature-target associations simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(3.5, 3))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "heatmap = sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=0.15, cbar_kws={\"shrink\": 0.35, \"aspect\": 25, \"label\": \"\"}, annot_kws={'size': 3.5})\n",
        "plt.title('Correlation Matrix', fontsize=8, fontweight='bold', pad=4)\n",
        "plt.xticks(fontsize=4.5, rotation=90)\n",
        "plt.yticks(fontsize=4.5, rotation=0)\n",
        "\n",
        "# Adjust colorbar font size\n",
        "cbar = heatmap.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By examining the quality row (or column) in the correlation matrix, we identify the strongest associations with wine quality. Alcohol content shows the strongest positive correlation (0.48), followed by sulphates (0.25) and citric acid (0.23). Conversely, volatile acidity exhibits the strongest negative correlation (-0.39), indicating that higher levels are associated with lower quality scores. Other features showing moderate negative correlations include total sulfur dioxide (-0.19) and density (-0.17). While quality is an ordinal variable, we compute Pearson correlation coefficients given its natural ordering, which allows us to capture linear trends. These correlations provide an initial indication of feature importance, though the classification models will ultimately determine the true discriminative power of each feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A4. PCA Variance Analysis\n",
        "\n",
        "The scree plot and cumulative explained variance plot provide detailed information about the variance explained by each principal component and the number of components needed to retain a specified percentage of the total variance.\n",
        "\n",
        "![PCA Variance Analysis](figures/pca_variance_analysis.png)\n",
        "\n",
        "*Figure A4: PCA Variance Analysis - Left: Scree plot showing explained variance per component; Right: Cumulative explained variance*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preparation and Feature Engineering\n",
        "\n",
        "Prior to model training, it is essential to prepare the data appropriately. Machine learning algorithms, particularly Support Vector Machines (SVM) and Artificial Neural Networks (ANN), are sensitive to the scale of input features. Features with larger numerical ranges can dominate the learning process, potentially biasing the model toward those features. Therefore, we apply feature normalization to ensure all features contribute equally to the model's learning process.\n",
        "\n",
        "### 3.1 Outlier Treatment and Normalization\n",
        "\n",
        "Our data preparation pipeline consists of two main steps: (1) outlier detection and treatment, and (2) feature standardization. Initial analysis revealed the presence of significant outliers across multiple features, particularly in residual sugar, chlorides, and sulphates. Outliers can significantly affect the mean and standard deviation used in standardization, potentially distorting the transformation. We identify outliers using the Interquartile Range (IQR) method, which is robust to extreme values. Outliers are capped (rather than removed) to preserve the sample size, ensuring we retain all 1,599 samples for model training. Subsequently, we apply standardization (Z-score normalization), which transforms features to have zero mean and unit variance. This standardized dataset will serve as the baseline for all classification models, ensuring fair comparison across different algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scree plot and cumulative explained variance plot (see Figure A4 in the appendix) reveal how many components are needed to capture the majority of the variance in the data. The scree plot reveals that the first principal component explains approximately 29% of the total variance, with subsequent components contributing progressively less. The cumulative variance plot indicates that 7 components are required to retain 90% of the variance, while 9 components capture 95% of the total variance. This suggests that dimensionality reduction is feasible, as most information can be preserved with fewer components than the original 11 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "\n",
        "# Create processed data directory\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Step 1: Outlier detection using IQR method\n",
        "print(\"OUTLIER DETECTION AND TREATMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "outliers_info = {}\n",
        "for feature in X.columns:\n",
        "    Q1 = X[feature].quantile(0.25)\n",
        "    Q3 = X[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = ((X[feature] < lower_bound) | (X[feature] > upper_bound)).sum()\n",
        "    outliers_info[feature] = {\n",
        "        'count': outliers,\n",
        "        'percentage': (outliers / len(X)) * 100,\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound\n",
        "    }\n",
        "\n",
        "# Print outlier summary\n",
        "outlier_summary = pd.DataFrame({\n",
        "    'Feature': list(outliers_info.keys()),\n",
        "    'Outliers': [outliers_info[f]['count'] for f in outliers_info.keys()],\n",
        "    'Percentage': [f\"{outliers_info[f]['percentage']:.2f}%\" for f in outliers_info.keys()]\n",
        "})\n",
        "print(outlier_summary.to_string(index=False))\n",
        "\n",
        "# Cap outliers instead of removing them (to preserve sample size)\n",
        "X_processed = X.copy()\n",
        "for feature in X.columns:\n",
        "    Q1 = X[feature].quantile(0.25)\n",
        "    Q3 = X[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    X_processed[feature] = X_processed[feature].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "print(f\"\\n✓ Outliers capped using IQR method (1.5 * IQR)\")\n",
        "print(f\"✓ Original dataset shape: {X.shape}\")\n",
        "print(f\"✓ Processed dataset shape: {X_processed.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Feature Standardization\n",
        "print(\"\\nFEATURE STANDARDIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_processed),\n",
        "    columns=X_processed.columns,\n",
        "    index=X_processed.index\n",
        ")\n",
        "\n",
        "# Combine scaled features with target\n",
        "df_normalized = pd.concat([X_scaled, y], axis=1)\n",
        "\n",
        "# Save normalized dataset\n",
        "df_normalized.to_csv('data/processed/winequality-red-normalized.csv', index=False)\n",
        "\n",
        "print(\"✓ Features standardized using StandardScaler (zero mean, unit variance)\")\n",
        "print(f\"✓ Normalized dataset saved to: data/processed/winequality-red-normalized.csv\")\n",
        "print(f\"\\nNormalized dataset statistics (first 5 features):\")\n",
        "print(X_scaled.describe().iloc[:, :5].round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The normalized dataset (`winequality-red-normalized.csv`) contains all original features standardized to have zero mean and unit variance, with outliers appropriately treated. This dataset will serve as the baseline for all classification models (SVM, ANN, and Random Forest), ensuring that feature scaling does not bias the learning process. The standardization process transforms each feature according to the formula:\n",
        "\n",
        "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "where $z$ is the standardized value, $x$ is the original value, $\\mu$ is the mean, and $\\sigma$ is the standard deviation of the feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Feature Interactions\n",
        "\n",
        "The correlation analysis revealed several strong pairwise relationships between features (|r| > 0.5), suggesting that these variables may interact in ways that influence wine quality. While linear models can capture individual feature effects, they may miss how the combination of features together affects the outcome. For example, the effect of fixed acidity on wine quality may depend on the level of citric acid present. To address this, we create multiplicative interaction features for the strongly correlated pairs identified in the correlation matrix. These interactions are created as multiplicative terms between standardized features, allowing us to evaluate whether explicitly modeling feature interactions improves classification performance compared to the baseline normalized dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create interaction features based on strong correlations (|r| > 0.5)\n",
        "print(\"FEATURE INTERACTIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Start with normalized features\n",
        "X_interactions = X_scaled.copy()\n",
        "\n",
        "# Define interaction pairs based on strong correlations from correlation matrix\n",
        "# Positive correlations (r > 0.5)\n",
        "interaction_pairs = [\n",
        "    ('fixed acidity', 'citric acid'),\n",
        "    ('fixed acidity', 'density', ''),\n",
        "    ('total sulfur dioxide', 'free sulfur dioxide', ''),\n",
        "    # Negative correlations (r < -0.5)\n",
        "    ('fixed acidity', 'pH', '_neg'),\n",
        "    ('citric acid', 'volatile acidity', '_neg'),\n",
        "    ('citric acid', 'pH', '_neg'),\n",
        "    ('alcohol', 'density', '_neg')\n",
        "]\n",
        "\n",
        "# Create interaction features\n",
        "interaction_names = []\n",
        "for feat1, feat2, suffix in interaction_pairs:\n",
        "    interaction_name = f\"{feat1}_{feat2}_interaction{suffix}\"\n",
        "    X_interactions[interaction_name] = X_scaled[feat1] * X_scaled[feat2]\n",
        "    interaction_names.append(interaction_name)\n",
        "\n",
        "print(f\"✓ Created {len(interaction_pairs)} interaction features:\")\n",
        "for name in interaction_names:\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "# Combine interaction features with target\n",
        "df_interactions = pd.concat([X_interactions, y], axis=1)\n",
        "\n",
        "# Save dataset with interactions\n",
        "df_interactions.to_csv('data/processed/winequality-red-interactions.csv', index=False)\n",
        "\n",
        "print(f\"\\n✓ Dataset with interactions saved to: data/processed/winequality-red-interactions.csv\")\n",
        "print(f\"✓ Original features: {X_scaled.shape[1]}\")\n",
        "print(f\"✓ Features with interactions: {X_interactions.shape[1]} (added {len(interaction_pairs)} interactions)\")\n",
        "print(f\"✓ Dataset shape: {df_interactions.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset with interactions (`winequality-red-interactions.csv`) contains all original standardized features plus interaction terms for strongly correlated feature pairs. This dataset allows us to evaluate whether capturing explicit feature interactions improves classification performance compared to the baseline normalized dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated variables called principal components. These components are ordered by the amount of variance they explain in the data. PCA serves two purposes in our analysis: (1) visualization of high-dimensional data in lower-dimensional space (2D/3D), which is mandatory for this project, and (2) as an alternative dataset for model training, allowing us to compare classification performance with and without dimensionality reduction.\n",
        "\n",
        "We apply PCA to the normalized dataset to ensure that all features contribute equally to the principal components. The scree plot and cumulative explained variance plot (see Figure A4 in the appendix) reveal how many components are needed to capture the majority of the variance in the data. The analysis shows that 7 components are required to retain 90% of the variance, while 9 components capture 95% of the total variance. This suggests that dimensionality reduction is feasible, as most information can be preserved with fewer components than the original 11 features. The visualization in 2D and 3D space helps us understand the separability of different quality classes in the reduced-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA to normalized features\n",
        "print(\"PRINCIPAL COMPONENT ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Fit PCA on normalized data\n",
        "pca = PCA()\n",
        "X_pca_full = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Determine number of components for 90% and 95% variance retention\n",
        "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
        "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "\n",
        "print(f\"Total features: {X_scaled.shape[1]}\")\n",
        "print(f\"\\nExplained variance by component:\")\n",
        "for i, (var, cum_var) in enumerate(zip(pca.explained_variance_ratio_, cumulative_variance), 1):\n",
        "    print(f\"  PC{i}: {var:.4f} ({var*100:.2f}%) | Cumulative: {cum_var:.4f} ({cum_var*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nComponents needed for 90% variance: {n_components_90}\")\n",
        "print(f\"Components needed for 95% variance: {n_components_95}\")\n",
        "\n",
        "# Visualize explained variance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scree plot\n",
        "axes[0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
        "             pca.explained_variance_ratio_, 'bo-', linewidth=2, markersize=8)\n",
        "axes[0].axhline(y=0.1, color='r', linestyle='--', label='10% threshold')\n",
        "axes[0].set_xlabel('Principal Component', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Explained Variance Ratio', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Scree Plot', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].legend()\n",
        "\n",
        "# Cumulative explained variance\n",
        "axes[1].plot(range(1, len(cumulative_variance) + 1), \n",
        "             cumulative_variance, 'go-', linewidth=2, markersize=8)\n",
        "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90% threshold')\n",
        "axes[1].axhline(y=0.95, color='red', linestyle='--', label='95% threshold')\n",
        "axes[1].set_xlabel('Number of Components', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Cumulative Explained Variance', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Cumulative Explained Variance', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/pca_variance_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA Visualization (2D and 3D) - MANDATORY\n",
        "print(\"\\nPCA VISUALIZATION (2D and 3D)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use first 2 and 3 components for visualization\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
        "\n",
        "pca_3d = PCA(n_components=3)\n",
        "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
        "\n",
        "# 2D Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 2D scatter plot\n",
        "scatter = axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='viridis', \n",
        "                          alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "axes[0].set_xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]*100:.2f}% variance)', \n",
        "                   fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]*100:.2f}% variance)', \n",
        "                   fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('PCA 2D Visualization', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(alpha=0.3)\n",
        "cbar = plt.colorbar(scatter, ax=axes[0])\n",
        "cbar.set_label('Quality Score', fontsize=10, fontweight='bold')\n",
        "\n",
        "# 3D Visualization\n",
        "ax_3d = fig.add_subplot(122, projection='3d')\n",
        "scatter_3d = ax_3d.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], \n",
        "                          c=y, cmap='viridis', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "ax_3d.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]*100:.2f}%)', fontsize=10, fontweight='bold')\n",
        "ax_3d.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]*100:.2f}%)', fontsize=10, fontweight='bold')\n",
        "ax_3d.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]*100:.2f}%)', fontsize=10, fontweight='bold')\n",
        "ax_3d.set_title('PCA 3D Visualization', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/pca_visualization_2d_3d.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ 2D visualization: PC1 ({pca_2d.explained_variance_ratio_[0]*100:.2f}%) + PC2 ({pca_2d.explained_variance_ratio_[1]*100:.2f}%) = {sum(pca_2d.explained_variance_ratio_)*100:.2f}% total variance\")\n",
        "print(f\"✓ 3D visualization: PC1 ({pca_3d.explained_variance_ratio_[0]*100:.2f}%) + PC2 ({pca_3d.explained_variance_ratio_[1]*100:.2f}%) + PC3 ({pca_3d.explained_variance_ratio_[2]*100:.2f}%) = {sum(pca_3d.explained_variance_ratio_)*100:.2f}% total variance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create PCA dataset for model training (using components that retain 95% variance)\n",
        "print(\"\\nCREATING PCA DATASET FOR MODEL TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "pca_model = PCA(n_components=n_components_95)\n",
        "X_pca_model = pca_model.fit_transform(X_scaled)\n",
        "\n",
        "# Create DataFrame with principal components\n",
        "pca_columns = [f'PC{i+1}' for i in range(n_components_95)]\n",
        "df_pca = pd.DataFrame(X_pca_model, columns=pca_columns, index=X_scaled.index)\n",
        "df_pca = pd.concat([df_pca, y], axis=1)\n",
        "\n",
        "# Save PCA dataset\n",
        "df_pca.to_csv('data/processed/winequality-red-pca.csv', index=False)\n",
        "\n",
        "print(f\"✓ PCA dataset created with {n_components_95} components (retaining {cumulative_variance[n_components_95-1]*100:.2f}% variance)\")\n",
        "print(f\"✓ PCA dataset saved to: data/processed/winequality-red-pca.csv\")\n",
        "print(f\"\\nPCA dataset shape: {df_pca.shape}\")\n",
        "print(f\"Original features: {X_scaled.shape[1]} → PCA components: {n_components_95} (reduction of {X_scaled.shape[1] - n_components_95} features)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The PCA visualization reveals the separability of different quality classes in the reduced-dimensional space. While perfect separation is not expected in 2D or 3D projections (as they capture only a portion of the total variance), the visualization provides insights into the underlying structure of the data. The PCA-transformed dataset (`winequality-red-pca.csv`) will be used as an alternative input for classification models, allowing us to compare performance with the normalized dataset and assess whether dimensionality reduction improves or hinders classification accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A3. Feature Correlations with Quality Score\n",
        "\n",
        "A detailed analysis focusing specifically on feature correlations with the quality score is presented below, providing a focused perspective on feature-target relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation with target variable (quality) - for appendix\n",
        "quality_corr = df.corr()['quality'].sort_values(ascending=False)\n",
        "quality_corr = quality_corr.drop('quality')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['red' if x < 0 else 'green' for x in quality_corr.values]\n",
        "bars = plt.barh(quality_corr.index, quality_corr.values, color=colors, alpha=0.7)\n",
        "plt.xlabel('Correlation with Quality', fontsize=12, fontweight='bold')\n",
        "plt.title('Feature Correlation with Quality Score', fontsize=14, fontweight='bold')\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (idx, val) in enumerate(quality_corr.items()):\n",
        "    plt.text(val + 0.01 if val > 0 else val - 0.01, i, f'{val:.3f}', \n",
        "             va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/quality_correlation_bar.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop features correlated with quality:\")\n",
        "print(quality_corr.abs().sort_values(ascending=False))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
