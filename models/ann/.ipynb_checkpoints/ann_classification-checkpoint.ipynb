{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb8b7d6-76e2-4b09-af5b-64d276ff1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2222e87-8df3-46c0-a84f-bad9feae3578",
   "metadata": {},
   "source": [
    "Inspired by biological neural networks in animal brains, Artificial Neural Networks (ANN) are the result of decades of computational model adaptations and discoveries to create a powerful computing system that mimics \"learning\" through example without specified rules. The invention of the perceptron model (1957, F. Rosenblatt), a method involving learnable weights and thresholds, laid key theoretical foundations that were integral to ANNs. Because single layer perceptrons were unable to handle the \"exclusive or\" (XOR) problem by effectively predicting the outputs of non-linearly seperable datasets. The introduction of backpropagation for training multiple-layered perceptrons allowed for networks to learn nonlinear decision boundaries by stacking multiple layers of neurons, or nodes.\n",
    "\n",
    "ANNs are comprised of at least three layers of nodes: an input layer (i) that recieves the raw features, at least one \"hidden\" layer (j) that performs intermediate transformation, and an output layer (k) that applies the activation function to produce a prediction. When connections are feedforward, as in multilayer perceptron (MLP) (informaiton moves in a single direction, layer by layer, with no feedback loop) and trained using a backpropagation algorithm. \n",
    "\n",
    "\n",
    "**Feedforward Computation**\n",
    "\n",
    "Each hidden node computes a scalar net activation using the weighted sum of the inputs.\n",
    "\n",
    "$$\n",
    "\\text{net}_j = \\sum_i x_i w_{ij} = \\mathbf{w}_j^\\top \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$w_ij$ is the input-to-hidden layer weight at node $j$, and $x$ is the input vector.\n",
    "\n",
    "An output for each hidden node is computed from a function *f* of its activation, where the activation function is a nonlinear function that is both continous and differentiable. Common choices include signmoid, tanh, and ReLU.\n",
    "\n",
    "$$\n",
    "z_j = f(net_j)\n",
    "$$\n",
    "\n",
    "Lastly, the output nodes calculate the net activation based on the outputs of the hidden nodes. \n",
    "$$\n",
    "\\text{net}_k = \\sum_{j=1}^{N_h} z_j w_{kj} = \\mathbf{w}_k^\\top \\mathbf{z}\n",
    "$$\n",
    "\n",
    "prediction $y_k$ is the activation of $net_k$. Often, for multi-class problems, softmax function is applied.\n",
    "\n",
    "\n",
    "**Backpropagation and Weight updates**\n",
    "\n",
    "Training involves minimizing the loss function.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}=-\\sum _{k=1}^Ky_k\\log (\\hat {y}_k)\n",
    "$$\n",
    "In the backpropagation pass, error is calculated by the difference between the predicted output from the network from the actual target. The gradients of the loss function with respect to each weight are applyed by using the chain rule in reverse, beginning with the output layer, to the hidden, and lastly through the input layer. The sensitivity of each node quantifies how much the error changes with respect to activition and are used to update the weights via gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "The output layer sensitivity is defined by:\n",
    "\n",
    "$$\n",
    "\\delta_k = \\hat{y}_k - y_k\n",
    "$$\n",
    "\n",
    "Hidden Layer sensitivity:\n",
    "\n",
    "$$\n",
    "\\delta_j = f'(net_j) \\sum_k \\delta_k w_{kj}\n",
    "$$\n",
    "\n",
    "Therefore, the weight gradient is calculated by:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = \\delta_j x_i\n",
    "$$\n",
    "\n",
    "Weights are updated using the gradient descent method, where $\\eta$ is the learning rate:\n",
    "\n",
    "$$\n",
    "w^{\\tau + 1} = w^{\\tau} - \\eta \\nabla E(w^{\\tau})\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38fa8564-5fc3-4197-8ef2-04b54d30b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../data/processed/winequality-red-normalized.csv')\n",
    "df_normalized = pd.read_csv('../../data/processed/winequality-red-normalized.csv')\n",
    "df_interactions = pd.read_csv('../../data/processed/winequality-red-interactions.csv')\n",
    "df_pca = pd.read_csv('../../data/processed/winequality-red-pca.csv')\n",
    "\n",
    "X_norm = df_normalized.drop('quality', axis=1)\n",
    "y_norm = df_normalized['quality']\n",
    "\n",
    "X_inter = df_interactions.drop('quality', axis=1)\n",
    "y_inter = df_interactions['quality']\n",
    "\n",
    "X_pca = df_pca.drop('quality', axis=1)\n",
    "y_pca = df_pca['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf309292-10f4-466d-a7bc-627017a4d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training ANN on Normalized dataset\n",
      "============================================================\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "\n",
      "Best parameters: {'activation': 'tanh', 'alpha': 1e-05, 'hidden_layer_sizes': (128, 64), 'learning_rate_init': 0.002, 'solver': 'adam'}\n",
      "Test Accuracy: 0.5969\n",
      "Macro F1-Score: 0.2925\n",
      "CV Macro F1: 0.2972 (+/- 0.1187)\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 0  0  1  1  0  0]\n",
      " [ 0  0 10  1  0  0]\n",
      " [ 0  0 94 41  1  0]\n",
      " [ 0  0 42 81  5  0]\n",
      " [ 0  0  1 23 16  0]\n",
      " [ 0  0  0  2  1  0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3     0.0000    0.0000    0.0000         2\n",
      "           4     0.0000    0.0000    0.0000        11\n",
      "           5     0.6351    0.6912    0.6620       136\n",
      "           6     0.5436    0.6328    0.5848       128\n",
      "           7     0.6957    0.4000    0.5079        40\n",
      "           8     0.0000    0.0000    0.0000         3\n",
      "\n",
      "    accuracy                         0.5969       320\n",
      "   macro avg     0.3124    0.2873    0.2925       320\n",
      "weighted avg     0.5743    0.5969    0.5788       320\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training ANN on Interactions dataset\n",
      "============================================================\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "\n",
      "Best parameters: {'activation': 'tanh', 'alpha': 1e-05, 'hidden_layer_sizes': (128, 64), 'learning_rate_init': 0.002, 'solver': 'adam'}\n",
      "Test Accuracy: 0.6062\n",
      "Macro F1-Score: 0.2993\n",
      "CV Macro F1: 0.2987 (+/- 0.1161)\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  0   0   1   1   0   0]\n",
      " [  0   0  10   1   0   0]\n",
      " [  0   0 100  35   1   0]\n",
      " [  0   0  45  76   7   0]\n",
      " [  0   0   2  20  18   0]\n",
      " [  0   0   0   2   1   0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3     0.0000    0.0000    0.0000         2\n",
      "           4     0.0000    0.0000    0.0000        11\n",
      "           5     0.6329    0.7353    0.6803       136\n",
      "           6     0.5630    0.5938    0.5779       128\n",
      "           7     0.6667    0.4500    0.5373        40\n",
      "           8     0.0000    0.0000    0.0000         3\n",
      "\n",
      "    accuracy                         0.6062       320\n",
      "   macro avg     0.3104    0.2965    0.2993       320\n",
      "weighted avg     0.5775    0.6062    0.5875       320\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training ANN on PCA dataset\n",
      "============================================================\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "\n",
      "Best parameters: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (128, 64), 'learning_rate_init': 0.002, 'solver': 'adam'}\n",
      "Test Accuracy: 0.5969\n",
      "Macro F1-Score: 0.2764\n",
      "CV Macro F1: 0.2887 (+/- 0.0400)\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 0  0  1  1  0  0]\n",
      " [ 0  0  9  2  0  0]\n",
      " [ 0  0 99 37  0  0]\n",
      " [ 0  0 43 81  4  0]\n",
      " [ 0  0  3 26 11  0]\n",
      " [ 0  0  0  2  1  0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           3     0.0000    0.0000    0.0000         2\n",
      "           4     0.0000    0.0000    0.0000        11\n",
      "           5     0.6387    0.7279    0.6804       136\n",
      "           6     0.5436    0.6328    0.5848       128\n",
      "           7     0.6875    0.2750    0.3929        40\n",
      "           8     0.0000    0.0000    0.0000         3\n",
      "\n",
      "    accuracy                         0.5969       320\n",
      "   macro avg     0.3116    0.2726    0.2764       320\n",
      "weighted avg     0.5748    0.5969    0.5722       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"Normalized\": (X_norm, y_norm),\n",
    "    \"Interactions\": (X_inter, y_inter),\n",
    "    \"PCA\": (X_pca, y_pca)\n",
    "}\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(64,), (64, 32), (128, 64)],\n",
    "    \"activation\": [\"logistic\", \"tanh\",\"relu\"],\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3],\n",
    "    \"learning_rate_init\": [5e-4, 1e-3, 2e-3],\n",
    "    \"solver\": [\"adam\"]\n",
    "}\n",
    "results = {}\n",
    "\n",
    "for name, (X, y) in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training ANN on {name} dataset\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Grid search\n",
    "    ann = MLPClassifier(max_iter=600, early_stopping=True, n_iter_no_change=20, random_state=42)\n",
    "    grid = GridSearchCV(\n",
    "        ann,\n",
    "        param_grid,\n",
    "        cv=5, \n",
    "        scoring=\"f1_macro\", \n",
    "        n_jobs=-1, \n",
    "        verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_ann = grid.best_estimator_\n",
    "    y_pred = best_ann.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    cv_scores = cross_val_score(best_ann, X_train, y_train, cv=5, scoring=\"f1_macro\")\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"best_params\": grid.best_params_,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1m,\n",
    "        \"cv_mean\": cv_scores.mean(),\n",
    "        \"cv_std\": cv_scores.std(),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "        \"report\": classification_report(y_test, y_pred, digits=4,zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Print\n",
    "    print(f\"\\nBest parameters: {grid.best_params_}\")\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1m:.4f}\")\n",
    "    print(f\"CV Macro F1: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "    print(\"\\nConfusion Matrix:\\n\", results[name][\"confusion_matrix\"])\n",
    "    print(\"\\nClassification Report:\\n\", results[name][\"report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed0b6bfa-f4b8-467a-9303-84cec960e5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANN Results Summary ===\n",
      "                                                    best_params  accuracy  \\\n",
      "Normalized    {'activation': 'tanh', 'alpha': 1e-05, 'hidden...  0.596875   \n",
      "Interactions  {'activation': 'tanh', 'alpha': 1e-05, 'hidden...  0.606250   \n",
      "PCA           {'activation': 'relu', 'alpha': 0.001, 'hidden...  0.596875   \n",
      "\n",
      "              f1_macro   cv_mean    cv_std  \n",
      "Normalized    0.292458  0.297194  0.059325  \n",
      "Interactions  0.299255  0.298743  0.058067  \n",
      "PCA           0.276351  0.288735  0.019996  \n"
     ]
    }
   ],
   "source": [
    "# Convert results dict to DataFrame\n",
    "summary = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "# Select only the columns you want in the table\n",
    "summary_table = summary[[\n",
    "    'best_params',\n",
    "    'accuracy',\n",
    "    'f1_macro',\n",
    "    'cv_mean',\n",
    "    'cv_std'\n",
    "]]\n",
    "\n",
    "print(\"\\n=== ANN Results Summary ===\")\n",
    "print(summary_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
